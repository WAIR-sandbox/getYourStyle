{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1cFzSVWeDMB"
      },
      "source": [
        "# Transform Face Dataset to Cropped Face dataset\n",
        "\n",
        "The goal of the Cropped dataset is to see if elimination of background and other noise in an image results in better classification performance.\n",
        "It is created by using a [Multi-task CNN](https://github.com/ipazc/mtcnn) face detector to detect face in an image and saving only the face as a new image.\n",
        "\n",
        "My Cropped dataset consists of the same classes and with the same number of images, contrary to HOG and SVM approach in the [paper](https://lib.jucs.org/article/104490/) \"Transfer Learning with EfficientNetV2S for Automatic Face Shape Classification\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install mtcnn\n",
        "!pip install kagglehub\n",
        "!pip install imutils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iT5xb9SWeDMC"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import os\n",
        "import numpy as np\n",
        "import imutils\n",
        "from mtcnn import MTCNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjmUMHuueDMC",
        "outputId": "dc0479fe-46cd-494d-bdcd-eb456ea956d5"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\n",
        "    handle=\"niten19/face-shape-dataset\",  # Replace with actual dataset name\n",
        "    # path=\".\"  # \".\" refers to the current directory\n",
        ")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpIsqcyOjAS0",
        "outputId": "b296387a-2e41-4b56-c096-55bda6cd39bf"
      },
      "outputs": [],
      "source": [
        "ds = os.path.join(path, 'FaceShape Dataset')\n",
        "print(ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqdbJ3V2eDMC"
      },
      "outputs": [],
      "source": [
        "def crop_ds(ds_name, sub_ds, class_name, image_format='jpg'):\n",
        "\n",
        "    detector = MTCNN()\n",
        "    min_conf = 0.8\n",
        "    offset = 20\n",
        "\n",
        "    os.makedirs('cropped_ds/'+sub_ds, exist_ok=True)\n",
        "    os.makedirs('cropped_ds/'+sub_ds+'/'+class_name, exist_ok=True)\n",
        "\n",
        "    # sometimes resources are exhausted and this script should be run multiple times\n",
        "    # to assure that no image is twice handled\n",
        "    # sets of files in both folders are compared\n",
        "    # only not transformed images are taken into account\n",
        "    image_list = glob.glob(ds_name+\"/\"+sub_ds+\"/\"+class_name+\"/*.\"+image_format)\n",
        "    new_image_list = ['/'.join(x.split('/')[1:]) for x in image_list]\n",
        "    cropped_list = glob.glob('cropped_ds'+\"/\"+sub_ds+\"/\"+class_name+\"/*.\"+image_format)\n",
        "    new_cropped_list = ['/'.join(x.split('/')[1:]) for x in cropped_list]\n",
        "    left = set(new_image_list) - set(new_cropped_list)\n",
        "    left_list = [x for x in left]\n",
        "    # print(left_list)\n",
        "    print(len(left_list))\n",
        "    for image_path in left_list:\n",
        "        new_im_path = os.path.join('cropped_ds', sub_ds, '/'.join(image_path.split('/')[-2:]))\n",
        "\n",
        "        # in case of error you know what image caused it\n",
        "        # print(new_im_path)\n",
        "        # print(image_path)\n",
        "        img = cv2.cvtColor(cv2.imread('/'+image_path), cv2.COLOR_BGR2RGB)\n",
        "        #  in case of very big image uncomment the line below\n",
        "        # img = imutils.resize(img, width=1280)\n",
        "        h,w,ch = img.shape\n",
        "        area = 0\n",
        "        final_face = None\n",
        "        detections = detector.detect_faces(img)\n",
        "        # transform only face with the biggest area\n",
        "        for det in detections:\n",
        "            if det['confidence'] >= min_conf:\n",
        "                x, y, width, height = det['box']\n",
        "               \n",
        "                object = img[max(y-offset,0):min(y+height+offset,h), max(0,x-offset):min(w,x+width+offset), :]\n",
        "                object_area = object.shape[0]*object.shape[1]\n",
        "                \n",
        "                if (object_area > area):\n",
        "                    area = object_area\n",
        "                    final_face = object\n",
        "        \n",
        "        cv2.imwrite(new_im_path, cv2.cvtColor(final_face, cv2.COLOR_RGB2BGR))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QUuNS3teDMC"
      },
      "source": [
        "It takes time and memory, so each transformation has its own cell. Test images are all in one cell, because their number is not too big."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "gwlEpl7GeDMD",
        "outputId": "8e19c03f-6548-4cbb-a279-0b187435e955"
      },
      "outputs": [],
      "source": [
        "crop_ds(ds, 'training_set', 'Heart')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "roVhdGsmeDMD"
      },
      "outputs": [],
      "source": [
        "crop_ds(ds, 'training_set', 'Oblong')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQ__BHdueDMD"
      },
      "outputs": [],
      "source": [
        "crop_ds(ds, 'training_set', 'Oval')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTaGB0j0eDMD"
      },
      "outputs": [],
      "source": [
        "crop_ds(ds, 'training_set', 'Round')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R74uT6FZeDMD"
      },
      "outputs": [],
      "source": [
        "crop_ds(ds, 'training_set', 'Square')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iS_Tp-wceDMD"
      },
      "outputs": [],
      "source": [
        "crop_ds(ds, 'testing_set', 'Heart')\n",
        "crop_ds(ds, 'testing_set', 'Oblong')\n",
        "crop_ds(ds, 'testing_set', 'Oval')\n",
        "crop_ds(ds, 'testing_set', 'Round')\n",
        "crop_ds(ds, 'testing_set', 'Square')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmN5fOzPeDMD"
      },
      "source": [
        "To test one image if something goes wrong"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 862
        },
        "id": "ZzQCwqAIeDMD",
        "outputId": "42aa02a1-b84f-4da1-b553-482102d4ebbc"
      },
      "outputs": [],
      "source": [
        "# show image with detections\n",
        "image_path = ds+'/training_set/Heart/heart (100).jpg'\n",
        "print(image_path)\n",
        "img = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
        "detector = MTCNN()\n",
        "detections = detector.detect_faces(img)\n",
        "detections\n",
        "img_with_dets = img.copy()\n",
        "min_conf = 0.9\n",
        "for det in detections:\n",
        "    if det['confidence'] >= min_conf:\n",
        "        x, y, width, height = det['box']\n",
        "        keypoints = det['keypoints']\n",
        "        cv2.rectangle(img_with_dets, (x-20,y-20), (x+width+20,y+height+20), (0,155,255), 2)\n",
        "        cv2.circle(img_with_dets, (keypoints['left_eye']), 2, (0,155,255), 2)\n",
        "        cv2.circle(img_with_dets, (keypoints['right_eye']), 2, (0,155,255), 2)\n",
        "        cv2.circle(img_with_dets, (keypoints['nose']), 2, (0,155,255), 2)\n",
        "        cv2.circle(img_with_dets, (keypoints['mouth_left']), 2, (0,155,255), 2)\n",
        "        cv2.circle(img_with_dets, (keypoints['mouth_right']), 2, (0,155,255), 2)\n",
        "plt.figure(figsize = (10,10))\n",
        "plt.imshow(img_with_dets)\n",
        "plt.axis('off')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 824
        },
        "id": "oH4lqnx3eDMD",
        "outputId": "7f87f0c5-1f6d-4734-b871-9b1db55c9f41"
      },
      "outputs": [],
      "source": [
        "# show cropped face\n",
        "offset = 20\n",
        "h,w,ch = img_with_dets.shape\n",
        "area = 0\n",
        "final_face = None\n",
        "for det in detections:\n",
        "    if det['confidence'] >= min_conf:\n",
        "        x, y, width, height = det['box']\n",
        "        object = img[max(y-offset,0):min(y+height+offset,h), max(0,x-offset):min(w,x+width+offset), :]\n",
        "        object_area = object.shape[0]*object.shape[1]\n",
        "        if (object_area > area):\n",
        "            area = object_area\n",
        "            final_face = object.copy()\n",
        "new_im_path = os.path.join('cropped_ds', 'train', '/'.join(image_path.split('/')[-2:]))\n",
        "plt.figure(figsize = (10,10))\n",
        "plt.imshow(final_face)\n",
        "plt.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeL3PfLSeDMD"
      },
      "source": [
        "## Data augmentation\n",
        "\n",
        "The augmentation method increases the number of images in the dataset while making it more difficult for the network to learn, because none of the images are completely standard. In the [paper](https://lib.jucs.org/article/104490/), the image variants include:\n",
        "- image rotation for a value between -50◦ and 30◦\n",
        "- adding Gaussian noise to an image\n",
        "- horizontal image mirroring\n",
        "- changing the contrast of an image for gamma value of 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0GxCz86eDME"
      },
      "outputs": [],
      "source": [
        "def augment_ds(ds_name, split_name, class_name, image_format='jpg'):\n",
        "\n",
        "    image_list = glob.glob(ds_name+\"/\"+ split_name + '/' +class_name+\"/*.\"+image_format)\n",
        "    print(image_list)\n",
        "    for image_path in image_list[:1]:\n",
        "        new_im_path = image_path[:-4]\n",
        "\n",
        "        img = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # flip image\n",
        "        flipped = cv2.flip(img, 1)\n",
        "        cv2.imwrite(new_im_path+'flipped.jpg', cv2.cvtColor(flipped, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "        # rotate image\n",
        "        angle = np.random.randint(-50, 30)\n",
        "        rotated = imutils.rotate(img, angle)\n",
        "        cv2.imwrite(new_im_path+'rotated.jpg', cv2.cvtColor(rotated, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "        # add Gaussian noise\n",
        "        mean = 0\n",
        "        std=1\n",
        "        noise = np.random.normal(mean, std, img.shape).astype(np.uint8)\n",
        "        noisy_image = cv2.add(img, noise)\n",
        "        cv2.imwrite(new_im_path+'gauss.jpg', cv2.cvtColor(noisy_image, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "        gamma = 0.8\n",
        "        invGamma = 1.0 / gamma\n",
        "        table = np.array([((i / 255.0) ** invGamma) * 255\n",
        "            for i in np.arange(0, 256)]).astype(\"uint8\")\n",
        "        # apply gamma correction using the lookup table\n",
        "        contrasted = cv2.LUT(img, table)\n",
        "        cv2.imwrite(new_im_path+'contrast.jpg', cv2.cvtColor(contrasted, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "        # plt.figure(figsize = (10,10))\n",
        "        # plt.imshow(img)\n",
        "        # plt.axis('off')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBFF6l9seDME",
        "outputId": "8bbfb693-9758-4e52-8067-cbd22e15871f"
      },
      "outputs": [],
      "source": [
        "augment_ds('cropped_ds', 'training_set', 'Heart')\n",
        "augment_ds('augmented_ds', 'training_set', 'Oblong')\n",
        "augment_ds('augmented_ds', 'training_set', 'Oval')\n",
        "augment_ds('augmented_ds', 'training_set', 'Round')\n",
        "augment_ds('augmented_ds', 'training_set', 'Square')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
